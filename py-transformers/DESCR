Transformers acts as the model-definition framework for
state-of-the-art machine learning with text, computer vision, audio,
video, and multimodal models, for both inference and training.

It centralizes the model definition so that this definition is agreed
upon across the ecosystem. transformers is the pivot across
frameworks: if a model definition is supported, it will be compatible
with the majority of training frameworks (Axolotl, Unsloth, DeepSpeed,
FSDP, PyTorch-Lightning, ...), inference engines (vLLM, SGLang, TGI,
...), and adjacent modeling libraries (llama.cpp, mlx, ...) which
leverage the model definition from transformers.

We pledge to help support new state-of-the-art models and democratize
their usage by having their model definition be simple, customizable,
and efficient.

There are over 1M+ Transformers model checkpoints on the Hugging Face
Hub you can use.

Explore the Hub today to find a model and use Transformers to help you
get started right away.
